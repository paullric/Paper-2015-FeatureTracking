%% Copernicus Publications Manuscript Preparation Template for LaTeX Submissions
%% ---------------------------------
%% This template should be used for the following class files: copernicus.cls, copernicus2.cls, copernicus_discussions.cls
%% The class files, the Copernicus LaTeX Manual with detailed explanations regarding the comments
%% and some style files are bundled in the Copernicus Latex Package which can be downloaded from the different journal webpages.
%% For further assistance please contact the Publication Production Office (production@copernicus.org).
%% http://publications.copernicus.org


%% Differing commands regarding the specific class files are highlighted.


%% copernicus.cls
%\documentclass[gmd]{copernicus}

%% copernicus2.cls
%\documentclass[gmd]{copernicus2}

%% copernicus_discussions.cls
\documentclass[gmdd, hvmath, online]{copernicus_discussions}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{setspace}
\usepackage{comment}
\usepackage{color}
\usepackage{float}
\usepackage{rotfloat}
\usepackage{rotating}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{newfloat}
\DeclareFloatingEnvironment[fileext=frm,placement={!ht},name=Algorithm]{algorithm}
\captionsetup[algorithm]{labelfont=bf}

% Custom commands
\newcommand{\vb}{\mathbf}
\newcommand{\vg}{\boldsymbol}
\newcommand{\mat}{\mathsf}
\newcommand{\diff}[2]{\frac{d #1}{d #2}}
\newcommand{\diffsq}[2]{\frac{d^2 #1}{{d #2}^2}}
\newcommand{\pdiff}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdiffsq}[2]{\frac{\partial^2 #1}{{\partial #2}^2}}


\begin{document}

\linenumbers

\title{TempestExtremes:  A Framework for Scale-Insensitive Pointwise Feature Tracking on Unstructured Grids}

\author{Paul A. Ullrich and Colin Zarzycki}

\affil{Paul A. Ullrich, Department of Land, Air and Water Resources, University of California, Davis, One Shields Ave., Davis, CA 95616.  Email: paullrich@ucdavis.edu}

\runningtitle{A Framework for Scale-Insensitive Pointwise Feature Tracking ...}

\runningauthor{P.~A.~Ullrich and C.~Zarzycki}

\correspondence{Paul A. Ullrich\\ (paullrich@ucdavis.edu)}



\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

%% These dates will be inserted by the Publication Production Office during the typesetting process.


\firstpage{1}

\maketitle  %% Please note that for the copernicus2.cls this command needs to be inserted after \abstract{TEXT}



\begin{abstract}
This paper describes the new pointwise feature tracking module in the TempestExtremes package.
\end{abstract}


%% only used for copernicus2.cls
%\abstract{
% TEXT
% \keywords{TEXT}}



\introduction  %% \introduction[modified heading if necessary]
%TEXT

%- GFDL tracker uses latitude-longitude arcs which can cause issues at high latitudes
%- cite Bosler paper, indicate algorithm does not currently support unstructured grids

The need for pointwise feature tracking has emerged as a major data processing challenge in climate science.  These ``trackers'' have been employed throughout the literature to answer scientific questions on the expected changes in important atmospheric features under climate change.

This paper presents a general software tool for pointwise feature tracking that...
\begin{itemize}
\item[] ...uses great-circle arcs for all distance calculations.  This avoids issues associated with latitude-longitude distance that emerges near the poles.
\item[] ...supports structured and unstructured grids.  This eliminates the need for post-processing of large native-grid output files and enables detection and characterization simultaneous with the model execution.
\item[] ...does not contain hard-coded variable names, so as to ensure robust applicability across reanalysis datasets.
\item[] ...allows for easy intercomparison of detection schemes by enabling all detection criteria to be specified on the command line.
\end{itemize}

As observed by Neu et al. and others, feature tracking schemes can produce wildly varying results depending on the specific choice of threshold variables and values.  Consequently, we argue that conclusions drawn from these tracking schemes should use an ensemble of detection schemes.

\subsection{Extratropical cyclones}

Manual counts of cyclones were performed by \cite{petterssen1956weather} in the Northern hemisphere from 1899-1939, and latter binned by \cite{klein1957principle} to determine the spatial distribution of such storms.  These techniques were later refined by \cite{whittaker1982atlas} by accounting for cyclone trajectories.  A similar survey in the Southern hemisphere was performed by \cite{taljaard1967development} for July 1957 - December 1958.  Manual tracking and characterization of cyclones was also performed by \cite{akyildiz1985systematic} using ECMWF forecast data for the 1981/82 winter.

One of the first automated detection and tracking for extratropical cyclones was developed by \cite{williamson1981storm} using nonlinear optimization to fit cyclonic profiles to anomalies in the 500-mb geopotential height field.  Storms were then tracked over a short forecast period using the best fit to the cyclone's centerpoint.

Counts of cyclones neglecting the cyclone trajectory were automatically generated from climate model output for both hemispheres by \cite{lambert1988cyclone} using local minima in 1000-hPa geopotential height.  This method had some shortcomings, including mischaracterization of local lows due to Gibbs' ringing and topographically-driven lows.  To overcome these problems, \cite{alpert1990climatological} proposed an additional minimum threshold on the local pressure gradient.  Similarly, \cite{le1990comparison} detected cyclones in ECMWF pressure data using a local minima in the sea-level pressure that must also be 4 mb below the average sea-level pressure of neighboring grid points, and must persist for three successive 6- or 12- hour intervals.

\cite{murray1991numerical} extracted low pressure centers from interpolated GCM data using local optimization, based on earlier work in \cite{rice1982durivation}.

Feature tracking on the sphere was revisited by \cite{hodges1995feature}, which extended tracking algorithms designed for Cartesian geometry \cite{hodges1994general} that were built from image processing techniques.

\subsection{Tropical cyclones}

Extratropical cyclone tracking techniques have been modified in order to support tropical cyclone tracking.  To eliminate ``false positives'' associated with extratropical cyclones and weak cyclonic depressions, many schemes require that the candidate be associated with a nearby warm core and be associated with a minimum threshold on surface winds for at least 1-3 days.  The definition of a ``warm core'' varies between modeling centers, including such options as air temperature anomaly on pressure surfaces \citep{Vitart1997,Zhao2009,Murakami2012}, geopotential thickness \citep{Tsutsui1996} and decay of vorticity with height \citep{Bengtsson2007,Strachan2013}.  Additional filtering of candidate storms over topography or within a specified latitudinal range may be required.  To better match observations, additional geographical, model or feature-dependent criteria may be applied \citep{Camargo2002,Walsh2007,Murakami2010,Murakami2012}.  In \cite{broccoli1990existing}, tropical cyclone-like features were detected by seeking local minima (occupying one grid point due to the coarse model resolution) in the surface pressure field that accompanied a minimum surface wind speed of 17 m/s.  A tracking procedure was also developed using a minimum distance between detections that represented a maximum velocity of 50 km/hr for TCs.  Tracking in \cite{haarsma1993tropical} instead detected maxima in the 850mb relative vorticity (greater than $3.5 \times 10^{-5} \mbox{s}^{-1}$ accompanied by a weak warm core (using local temperature anomaly at 250mb, 500mb and 850mb) and required TCs to exist for a minimum of 3 days.  \cite{bengtsson1995hurricane} made use of thresholds on 850mb relative vorticity, maximum local velocity, temperature anomaly at 700, 500 and 300 hPa and required systems exist for at least 1.5 days.  \cite{tsutsui1996simulated} detects geopotential height minima at 1000mb and then imposes additional thresholds on 900mb relative vorticity, divergence, vertical pressure velocity, geopotential thickness and zonal wind velocity.  \cite{camargo2002improving} use the absolute value of relative vorticity for detection, then further employ thresholds on maximum wind speed, sea level pressure, temperature anomaly, mean wind speed and track length.

A tabulated overview of the thresholds utilized by many of these schemes can be found in \cite{walsh2007objectively}, along with several proposed guidelines on detection schemes.  Nonetheless, the definition of an optimal objective criteria for atmospheric features has eluded development, suggesting that there may be no singular criteria capable of both perfect detection and zero false positive rate.  It is widely acknowledged that weaker tropical storms are difficult to track, and the observational record of these less-intense, short-lived storms is questionable \citep{Landsea2010}.

\subsection{African Easterly Waves}

A similar manual study to \cite{akyildiz1985systematic} was performed by \cite{reed1988evaluation} to detect and track African easterly waves.

Storm tracks and African easterly waves were tracked by \cite{hodges2003comparison}.

\cite{konig1993objective} uses minima in the 1000 hPa geopotential height to identify cyclones with associated maxima in the 850 hPa vorticity.

%tunability of parameters, support for unstructured grids, works at all latitudes
%can be used for both TCs and ETCs (even AEWs)

\subsection{Building blocks of detection schemes}

Most of the detection and characterization algorithm that are present in the literature share a common detection procedure:
\begin{itemize}
\item[1.] Identify an initial set of candidate points by finding local extrema.  Local extrema can be further isolated, for instance by requiring that the local extrema be sufficiently anomalous when contrasted with their neighbors.  For most cyclonic structures, either minima in the sea level pressure field or maxima in the absolute value of the relative vorticity are used.
\item[2.] Eliminate candidate points that do not satisfy a prescribed set of thresholds.  For instance, tropical cyclones typically require the presence of an upper-level warm core that is sufficiently near the sea level pressure minima.
\item[3.] Connect candidate points together in time to generate feature paths, eliminating paths that are of insufficient length or do not meet additional criteria.
\end{itemize}

At high horizontal resolutions, climate datasets become increasingly unwieldy to store and analyze.  Simultaneously, single-core computing systems have effectively reached peak performance, and so  supercomputing systems have been forced to adopt massively parallel infrastructure.  To address the need for feature tracking in multi-decadal high-resolution ensemble datasets, algorithms based on MapReduce have been developed \citep{Prabhat2012,Zarzycki2014}.  In this case, candidates are first isolated on individual time slices (in an embarrassingly parallel manner), and then stitched across time to build trajectories.  For detection codes based on MapReduce, parallel performance is usually bottlenecked by file system access times (I/O).

\section{Algorithms}

This section describes the building blocks that have been utilized in constructing our detection and characterization framework.  As mentioned earlier, in order to avoid sensitivity of the detection scheme to grid resolution, great circle distance has been employed throughout.  In terms of regular latitude-longitude coordinates, great circle distance between points $(\lambda_1, \varphi_1)$ and $(\lambda_2, \varphi_2)$ is defined via the symmetric operation
\begin{equation}
r(\lambda_1, \varphi_1; \lambda_2, \varphi_2) = a \arccos \left( \sin \varphi_1 \sin \varphi_2 + \cos \varphi_1 \cos \varphi_2 \cos (\lambda_1 - \lambda_2) \right).
\end{equation}  Algorithmically, this calculation is implemented as \texttt{gcdist(p,q)} for given graph nodes \texttt{p} and \texttt{q}.

\subsection{Efficient neighbor search using $k$-d Trees}

Three-dimensional ($k=3$) $k$-d trees \citep{bentley1975multidimensional} are used throughout our detection code using the implementation of \cite{tsiombikas2015kdtree}.  Although $k$-d trees use straight-line instance instead of great circle distance, we utilize the observation that straight-line and great-circle distance maintain the same ordering for points confined to the surface of the sphere.  In particular, we utilize three key functions made available by the $k$-d tree implementation:
\begin{itemize}
\item[] \texttt{K = build\_kd\_tree(P)} constructs a $k$-d tree \texttt{K} from a point set \texttt{P}.
\item[] \texttt{q = kd\_tree\_nearest\_neighbor(K, p)} locates the nearest neighbor \texttt{q} to point \texttt{p} using the $k$-d tree \texttt{K}.
\item[] \texttt{S = kd\_tree\_all\_neighbors(K, p, dist)} locates all points that are within a distance \texttt{dist} of a point \texttt{p} within the $k$-d tree \texttt{K}.
\end{itemize}

\noindent A key advantage of KD trees is their relatively efficient $O(n \log n)$ construction time and $O(\log n)$ average time nearest neighbor search.

\subsection{Unstructured grid specification}

For purposes of determining connectivity of the unstructured grid, we require the specification of a graph such as the one depicted in Figure \ref{fig:unstructured_grid}.  The connectivity information is stored textually as an adjacency list via a variable-length comma-separated variable file.  The total number of nodes ($N$) is specified at the top of the file, followed by $N$ lines containing the longitude (lon), latitude (lat), number of adjacent nodes, and finally a 1-indexed list of all adjacent nodes, such as depicted below:
\ \\

\noindent \begin{tabular}{|p{\textwidth}|}
\hline \small \begin{verbatim}
<total number of nodes>
<lon>,<lat>,<# adj. nodes>,<first adj. node>,...,<last adj. node>
...
\end{verbatim} \\
\hline
\end{tabular}

\begin{figure}
\begin{center}
\includegraphics[width=4in]{UnstructuredGrid.pdf}
\end{center}
\caption{An example adjacency graph describing an unstructured grid (blue lines), where nodes are co-located with volume centerpoint locations (solid circles) and edges connect adjacent volumes.} \label{fig:unstructured_grid}
\end{figure}

\subsection{Extrema detection}

For purposes of computational efficiency, candidate points are initially located by identifying local extrema in a given field (for instance SLP) via \texttt{find\_all\_minima} (Algorithm \ref{alg:find_all_minima}).  Candidates are then eliminated if they are ``too close'' to stronger extrema (Algorithm \ref{alg:merge_candidates_minima}).  The initial search field is specified to TempestExtremes either via the \texttt{--searchbymin} or \texttt{--searchbymax} command line argument.  The merge distance used in \texttt{merge\_candidates\_minima} is specified via the \texttt{--mergedist} command line argument.

\begin{algorithm}
\caption{Locate the set of all nodes \texttt{P} that are local minima for a field \texttt{G} (for instance, SLP) defined on an unstructured grid.  The procedure for locating maxima is analogous.\ \\} \label{alg:find_all_minima}
\noindent \begin{tabular}{p{5in}}
\hline \small \begin{verbatim}
set P = find_all_minima(field G)
  for each node f
    is_minima[f] = true
    for each neighbor node v of f
      if G[v] < G[f] then
        is_minima[f] = false
    if is_minima[f] then
      insert f into P
\end{verbatim} \\
\hline
\end{tabular}
\end{algorithm}

\begin{algorithm}
\caption{Given a field \texttt{G} defined on an unstructured grid and a set of candidate points \texttt{P}, remove candidate minima that are within a distance \texttt{dist} of a more extreme minimum, and return the new candidate set \texttt{Q}.\ \\} \label{alg:merge_candidates_minima}
\noindent \begin{tabular}{p{5in}}
\hline \small \begin{verbatim}
set Q = merge_candidates_minima(field G, set P, dist)
  K = build_kd_tree(P)
  for each candidate p in P
    retain_p = true
    N = kd_tree_all_neighbors(K, p, dist)
    for all q in N
      if (G[q] < G[p]) then retain_p = false
    if retain_p then insert p into Q
\end{verbatim} \\
\hline
\end{tabular}
\end{algorithm}

\subsection{Closed contour criteria}

Although a first pass at candidate points may be made by looking for local extrema (comparing against all neighboring nodes), this criteria is not robust across model resolution.  That is, the distance between a node and its neighbors decreases proportional to the local grid spacing, and so does not definite ``physical'' criterion.  Consequently, we instead advocate for a \textit{closed contour criteria} to define candidate nodes.  Closed contours were first employed by \cite{bell198915}, who used a 30m 500 mb geopotential height contour to identify closed circulation centers.  Their approach used radial arms generated at 15$^\circ$ intervals over a great circle distance of 2$^\circ$ and required that geopotential heights rise by at least 30m along each arm.  Unfortunately, the use of radial arms to define the closed contour is again sensitive to model resolution, since it has the potential to only sample as many neighbors as radial arms employed.

Here we propose an alternative closed contour criteria that is largely insensitive to model resolution that uses graph search to ensure that all paths along the unstructured grid from an initial location \texttt{p0} lead to a sufficiently large decrease (or increase) in a given field \texttt{G}.  This criteria is illustrated in Figure \ref{fig:ClosedContour}, and is implemented in Algorithm \ref{alg:find_max_near} and \ref{alg:closed_contour_max} (for closed contours around local maxima).  An analogous algorithm \texttt{no\_closed\_contour} is also provided, which has similar functionality but discards candidates that satisfy the closed contour criteria (this may be desirable, for instance, to identify cyclonic structures that do not have a warm core).

\begin{algorithm}
\caption{Find the node \texttt{pmax} containing the maximal value of the field \texttt{G} within a distance \texttt{maxdist} of the node \texttt{p}.  An analogous procedure \texttt{find\_min\_near} is provided for locating nodes containing minimal values of the field.\ \\} \label{alg:find_max_near}
\noindent \begin{tabular}{p{5in}}
\hline \small \begin{verbatim}
node pmax = find_max_near(node p, field G, maxdist)
  set visited = {}
  set tovisit = {p}
  pmax = p
  while tovisit is not empty
    q = remove node from tovisit
    if (q in visited) then continue
    add q to visited
    if (gcdist(p,q) > maxdist) then continue
    if (G[q] > G[pmax]) then pmax = q
\end{verbatim} \\
\hline
\end{tabular}
\end{algorithm}

\begin{algorithm}
\caption{Determine if there is a closed contour in field \texttt{G} of magnitude \texttt{thresh} around the point \texttt{p0}, defined by \texttt{p0 = find\_max\_near(p, G, maxdist)}, within distance \texttt{dist}.  That is, along all paths away from \texttt{p0}, the field \texttt{G} must drop by at least \texttt{thresh} within distance \texttt{dist}.  The closed contour criteria is depicted in Figure \ref{fig:ClosedContour}.  An analogous procedure is defined for closed contours around minima.\ \\} \label{alg:closed_contour_max}
\noindent \begin{tabular}{p{5in}}
\hline \small \begin{verbatim}
closed_contour_max(point p, field G, dist, maxdist, thresh)
  p0 = find_max_near(p, G, maxdist)
  set visited = {}
  set tovisit = {p0}
  while tovisit is not empty
    q = remove point from tovisit
    if (q in visited) then continue
    add q to visited
    if (gcdist(p0,q) > dist) then return false
    if (G[p0] - G[q] < thresh) then
      add all neighbors of q to tovisit
  return true
\end{verbatim} \\
\hline
\end{tabular}
\end{algorithm}


\begin{figure}[H]
\begin{center}
\includegraphics[width=5in]{ClosedContourCriteria2.pdf}
\end{center}
\caption{An illustration of the closed contour criteria.  Nodes shaded in white (gray) satisfy (do not satisfy) the threshold of the field value at \texttt{p0}.  Since only edge-neighbors are included, \texttt{B} constitutes a boundary to the interior of the closed contour.  Because \texttt{A} lays outside the solid circle, the contour with distance $d_0$ is not a closed contour, whereas the dashed contour with distance $d_1$ does satisfy the closed contour criteria.} \label{fig:ClosedContour}
\end{figure}

\subsection{Thresholding}

Additional threshold criteria may be applied at the Map() stage in order to further eliminate undesirable candidates.  For example, a common threshold criteria requires that a field \texttt{G} satisfy some minimum value within a distance \texttt{dist} of the candidate, as implemented in Algorithm \ref{alg:threshold_max}.

\begin{algorithm}
\caption{Determine if a candidate node \texttt{p} satisfies the requirement that there exists another node \texttt{p0} within distance \texttt{dist} of \texttt{p} with \texttt{G[p] $>$ thresh}.\ \\} \label{alg:threshold_max}
\noindent \begin{tabular}{p{5in}}
\hline \small \begin{verbatim}
threshold_max(node p, field G, dist, thresh)
  p0 = find_max_near(p, G, dist)
  if (G[p0] < thresh) then
    return false
  else
    return true
\end{verbatim} \\
\hline
\end{tabular}
\end{algorithm}

\subsection{Stitching}

The basic stitching procedure is implemented in Algorithm \ref{alg:stitch_nodes}.

\begin{algorithm}
\caption{Determine all feature paths \texttt{S}, given array of nodes \texttt{P$[$1..T$]$} and maximum great-circle distance between nodes at subsequent time levels \texttt{dist}.\ \\} \label{alg:stitch_nodes}
\noindent \begin{tabular}{p{5in}}
\hline \begin{verbatim}
path set S = stitch_nodes(set array P[1..T], dist)
  for each time level t = 1..T
    K[t] = build_kd_tree(P[t])
  for each time level t = 1..T
    while P[t] is not empty
      initialize empty path s
      p = remove next candidate from P[t]
      add p into s
      for time level u = t+1..T
        q = kd_tree_nearest_neighbor(K[u], p)
        if (q in P[u]) and (gcdist(p,q) < dist) then
          add q into s
          remove q from P[u]
          p = q
        else break
      add s into S
\end{verbatim} \\
\hline
\end{tabular}
\end{algorithm}

\section{Selected examples}

A few selected examples of the feature detection tool are now provided.  The first three examples use data from the NCEP Climate Forecast System Reanalysis (CFSR), available at 0.5 degree global resolution with 6-hourly output from 1979-present \citep{saha2010ncep}.

\subsection{Tropical cyclones in CFSR} \label{sec:TropicalCycloneExample}

The command line we use to detect tropical cyclone-like features in CFSR is provided below.  Three-dimensional (time + 2D space) hyperslabs of CFSR data have been extracted, with \texttt{TMP\_L100} corresponding to 400hPa air temperature, and \texttt{U\_GRD\_L100} and \texttt{V\_GRD\_L100} corresponding to 850hPa zonal and meridional velocities.  Candidates are initially identified by minima in the sea level pressure (\texttt{PRMSL\_L101}), and then eliminated if a smaller minimum exists within a great circle distance of 2.0 degrees.  The closed contour criteria is then applied, requiring an increase in SLP of at least 200Pa over a distance of 4 degrees away from the candidate node, and a decrease in 400hPa air temperature of 0.4K within 8 degrees of the node within 1.1 degrees of the candidate with maximum air temperature.  Since CFSR is on a structured latitude-longitude grid, the output format is \texttt{i,j,lon,lat,psl,maxu,zs}, where \texttt{i,j} are the longitude and latitude coordinates within the dataset, \texttt{lon,lat} are the actual longitude and latitude of the candidate, \texttt{psl} is the SLP at the candidate point (equal to the maximum SLP within 0 degrees of the candidate), \texttt{maxu} is the vector magnitude of the maximum 850 hPa wind within 4 degrees of the candidate, and \texttt{zs} is the topographic height at the candidate point.

{\small \begin{verbatim}
  ./DetectCyclonesUnstructured    --in_data "$uvfile;$tpfile;$hfile" --out $outf    --searchbymin PRMSL_L101    --mergedist 2.0    --closedcontourcmd "PRMSL_L101,200.,4,0;
       TMP_L100,-0.4,8.0,1.1"    --outputcmd "PRMSL_L101,max,0;
       _VECMAG(U_GRD_L100,V_GRD_L100),max,4;
       HGT_L1,max,0"
\end{verbatim}}

All outputs from DetectCyclonesUnstructured are then concatenated into a single file containing candidates at all times (\texttt{pgbhnl.dcu\_tc\_all.dat}).  Candidates are then stitched in time to form paths, with a maximum distance between candidates of 8.0 degrees (great circle distance), consisting of at least 8 candidates per path, and with a maximum gap size of 2 (most consecutive timesteps with no associated candidate).  Because localized low-pressure regions that are unrelated to tropical cyclones can form as a consequence of topographic forcing, we also require that for at least 8 time steps the underlying topographic height (\texttt{zs}) be at most 100 meters.  The associated command line for StitchNodes is:

{\small \begin{verbatim}
  ./StitchNodes    --in pgbhnl.dcu_tc_all.dat    --out pgbhnl.dcu_tc_stitch.dat    --format "i,j,lon,lat,psl,maxu,zs"    --range 8.0 --minlength 8 --maxgap 2    --threshold "zs,<=,100.0,8"
\end{verbatim}}

Once the complete set of tropical cyclone paths have been computed, total tropical cyclone density over each 0.5 degree grid cell is plotted in Figure \ref{fig:TropicalCycloneDensity}.  Overall the results show very good agreement with reference fields ({\color{red}Citation?}).

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4in, clip, trim=0.2cm 3.6cm 0.2cm 3.1cm]{plot-cfsr_tc_density.pdf}
\end{center}
\caption{Tropical cyclone count over the period 1979-2010 obtained using the procedure described in section \ref{sec:TropicalCycloneExample}.} \label{fig:TropicalCycloneDensity}
\end{figure}

\subsection{Extratropical cyclones in CFSR} \label{sec:ExtratropicalCycloneExample}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4in, clip, trim=0.2cm 3.6cm 0.2cm 3.1cm]{plot-cfsr_etc_density.pdf}
\end{center}
\caption{Extratropical cyclone count over the period 1979-2010 obtained using the procedure described in section \ref{sec:ExtratropicalCycloneExample}.} \label{fig:ExtratropicalCycloneDensity}
\end{figure}

\subsection{Tropical easterly waves in CFSR} \label{sec:TropicalEasterlyWavesExample}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4in, clip, trim=0.2cm 3.6cm 0.2cm 3.1cm]{plot-cfsr_tew_density.pdf}
\end{center}
\caption{Tropical easterly wave density over the period 1979-2010 obtained using the procedure described in section \ref{sec:TropicalEasterlyWaveExample}.} \label{fig:TropicalEasterlyWaveDensity}
\end{figure}


\subsection{Tropical cyclones in a simulation with variable-resolution CESM}

\conclusions

\pagebreak
\begin{acknowledgements}

\end{acknowledgements}
\pagebreak

\appendix
\subsection{Software Documentation: DetectCyclonesUnstructured}

\begin{verbatim}
Usage: DetectCyclonesUnstructured <parameter list>
Parameters:
  --in_data <string> [""] 
  --in_connect <string> [""] 
  --out <string> [""] 
  --searchbymin <string> [""] (default PSL)
  --searchbymax <string> [""] 
  --maxlat <double> [0.000000] (degrees)
  --minlat <double> [0.000000] (degrees)
  --topofile <string> [""] 
  --maxtopoht <double> [0.000000] (m)
  --mergedist <double> [0.000000] (degrees)
  --closedcontourcmd <string> [""] [var,dist,delta,minmaxdist;...]
  --noclosedcontourcmd <string> [""] [var,dist,delta,minmaxdist;...]
  --thresholdcmd <string> [""] [var,op,value,dist;...]
  --outputcmd <string> [""] [var,op,dist;...]
  --timestride <integer> [1] 
  --regional <bool> [false] 
  --out_header <bool> [false] 
  --verbosity <integer> [0] 
\end{verbatim}

\begin{itemize}
\item[] \texttt{--in\_data <string>} \\ The input datafile in NetCDF format.
\item[] \texttt{--in\_connect <string>} \\ A connectivity file, which uses a vertex list to describe the graph structure of the input grid.  This parameter is not required if the data is on a latitude-longitude grid.
\item[] \texttt{--out <string>} \\ The output file containing the filtered list of candidates in plain text format.
\item[] \texttt{--searchbymin <string>} \\ The input variable to use for initially selecting candidate points (defined as local minima).  By default this is ``PSL'', representing detection of surface pressure minima.  Only one of \texttt{searchbymin} and \texttt{searchbymax} may be set.
\item[] \texttt{--searchbymax <string>} \\ The input variable to use for initially selecting candidate points (defined as local maxima).  Only one of \texttt{searchbymin} and \texttt{searchbymax} may be set.
\item[] \texttt{--maxlat <double>} \\ The maximum absolute latitude for candidate points.  Candidates at higher latitudes are discarded.
\item[] \texttt{--minlat <double>} \\ The minimum absolute latitude for candidate points.  Candidates at lower latitudes are discarded.
\item[] \texttt{--topofile <string>} \\ An auxiliary file containing topographic information.
\item[] \texttt{--maxtopoht <double>} \\ If \texttt{--topofile} is specified, discard candidates over topography higher than specified by this parameter.
\item[] \texttt{--mergedist <double>} \\ Merge candidate points with distance (in degrees) shorter than the specified value.  Among two candidates within the merge distance, only the candidate with lowest \texttt{searchbymin} or highest \texttt{searchbymax} value will be retained. 
\item[] \texttt{--closedcontourcmd <cmd1>;<cmd2>;...} Eliminate candidates if they do not have a closed contour.  Closed contour commands are separated by a semi-colon.  Each closed contour command takes the form \texttt{var,dist,delta,pivotdist}.  These arguments are as follows.
\begin{itemize}
\item[] \texttt{var <variable>}  The variable used for the contour search.
\item[] \texttt{dist <double>}  The great-circle distance (in degrees) from the pivot within which the closed contour criteria must be satisfied.
\item[] \texttt{delta <double>}  The amount by which the field must change from the pivot value.  If positive (negative) the field must increase (decrease) by this value along the contour.
\item[] \texttt{pivotdist <double>}  The distance away from the candidate to search for the pivot.  If \texttt{delta} is positive (negative), the pivot is a local minimum (maximum).
\end{itemize}
\item[] \texttt{--noclosedcontourcmd <cmd1>;<cmd2>;...} \\ As \texttt{closedcontourcmd}, except eliminates candidates if a closed contour is present.
\item[] \texttt{--thresholdcmd <cmd1>;<cmd2>;...}  Eliminate candidates that do not satisfy a threshold criteria (there must exist a point within a given distance of the candidate that satisfies a given equality or inequality).  Threshold commands are separated by a semi-colon.  Each threshold command takes the form \texttt{var,op,value,dist}.  These arguments are as follows.
\begin{itemize}
\item[] \texttt{var <variable>}  The variable used for the contour search.
\item[] \texttt{op <string>}  Operator that must be satisfied for threshold (options include \texttt{>}, \texttt{>=}, \texttt{<}, \texttt{<=}, \texttt{=}, \texttt{!=}).
\item[] \texttt{value <double>}  The value on the RHS of the comparison.
\item[] \texttt{dist <double>}  The great circle distance away from the candidate to search for a point that satisfies the threshold (in degrees).
\end{itemize}
\item[] \texttt{--outputcmd <cmd1>;<cmd2>;...}  Include additional columns in the output file.  Output commands take the form \texttt{var,op,dist}. These arguments are as follows.
\begin{itemize}
\item[] \texttt{var <variable>}  The variable used for the contour search.
\item[] \texttt{op <string>}  Operator that is applied over all points within the specified distance of the candidate (options include \texttt{max}, \texttt{min}, \texttt{avg}, \texttt{maxdist}, \texttt{mindist}).
\item[] \texttt{dist <double>}  The great circle distance away from the candidate wherein the operator is applied (in degrees).
\end{itemize}
\item[] \texttt{--timestride <integer>} \\ Only examine discrete times at the given stride (by default 1).
\item[] \texttt{--regional} \\ When a latitude-longitude grid is employed, do not consider longitudinal boundaries to be periodic.
\item[] \texttt{--out\_header} \\ Output a header describing the columns of the data file.
\item[] \texttt{--verbosity <integer>} \\ Set the verbosity level (default 0).
\end{itemize}

\subsubsection{Variable Specification}

Quantities of type \texttt{<variable>} include both NetCDF variables in the input file (for example, ``Z850'') and simple operations performed on those variables.  By default it is assumed that NetCDF variables are specified in the \texttt{.nc} file as
\begin{center}
\texttt{float Z850(time, lat, lon)} \quad or \quad \texttt{float Z850(time, ncol)}
\end{center} for structured latitude-longitude grids and unstructured grids, respectively.  If variables have no time variable, they have the related specification
\begin{center}
\texttt{float Z850(lat, lon)} \quad or \quad \texttt{float Z850(ncol)}
\end{center}  If variables include an additional dimension, for instance,
\begin{center}
\texttt{float Z(time, lev, lat, lon)} \quad or \quad \texttt{float Z(time, lev, ncol)}
\end{center} they may be specified on the command-line as $\texttt{Z(<lev>)}$, where the integer index \texttt{<lev>} corresponds to the first dimension (or the dimension after \texttt{time}, if present).  

Simple operations on variables are also supported, including
\begin{itemize}
\item[] \texttt{\_VECMAG(<variable>, <variable>)} 2-component vector magnitude,
\item[] \texttt{\_PLUS(<variable>, <variable>)} Pointwise sum of variables,
\item[] \texttt{\_DIFF(<variable>, <variable>)} Pointwise difference of variables.
\end{itemize}  The following are valid examples of \texttt{<variable>} type,
\begin{center}
\texttt{\_VECMAG(U850, V850)} \quad and \quad \texttt{\_DIFF(U(3),U(5))}.
\end{center}

\subsection{Software Documentation: StitchNodes}

\begin{verbatim}
Usage: StitchNodes <parameter list>
Parameters:
  --in <string> [""] 
  --out <string> [""] 
  --format <string> ["no,i,j,lon,lat"] 
  --range <double> [5.000000] (degrees)
  --minlength <integer> [3] 
  --min_endpoint_dist <double> [0.000000] (degrees)
  --min_path_dist <double> [0.000000] (degrees)
  --maxgap <integer> [0] 
  --threshold <string> [""] [col,op,value,count;...]
  --timestride <integer> [1] 
  --out_format <string> ["std"] (std|visit)
\end{verbatim}

\begin{itemize}
\item[] \texttt{--in <string>} \\ The input file (a list of candidates from DetectCyclonesUnstructured).
\item[] \texttt{--out <string>} \\ The output file containing the filtered list of candidates in plain text format.
\item[] \texttt{--format <string>} \\ The structure of the columns of the input file.
\item[] \texttt{--range <double>} \\ The maximum distance between candidates along a path.
\item[] \texttt{--minlength <integer>} \\ The minimum length of a path (in terms of number of discrete times).
\item[] \texttt{--min\_endpoint\_dist <double>} \\ The minimum great-circle distance between the first candidate on a path and the last candidate (in degrees).
\item[] \texttt{--min\_path\_dist <double>} \\ The minimum path length, defined as the sum of all great-circle distances between candidate nodes (in degrees).
\item[] \texttt{--maxgap <integer>} \\ The largest gap (missing candidate nodes) along the path (in discrete time points).
\item[] \texttt{--threshold <cmd1>;<cmd2>;...} \\  Eliminate paths that do not satisfy a threshold criteria (a specified number of candidates along path must satisfy an equality or inequality).  Threshold commands are separated by a semi-colon.  Each threshold command takes the form \texttt{col,op,value,count}.  These arguments are as follows.
\begin{itemize}
\item[] \texttt{col <integer>}  The column in the input file to use in the threshold criteria.
\item[] \texttt{op <string>}  Operator used for comparison of column value (options include \texttt{>}, \texttt{>=}, \texttt{<}, \texttt{<=}, \texttt{=}, \texttt{!=}).
\item[] \texttt{value <double>}  The value on the right-hand-side of the operator. 
\item[] \texttt{count <integer>}  The minimum number of candidates along the path that must satisfy this criteria.
\end{itemize}
\item[] \texttt{--timestride <integer>} \\ Only examine discrete times at the given stride (by default 1).
\end{itemize}



\bibliography{FeatureTrackingBibliography}
\bibliographystyle{copernicus}

\end{document}
